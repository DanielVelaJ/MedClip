{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e8e534-f7a2-4e0c-a896-332a02b70764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.pipelinesV2 import build_pipeline,build_coco_pipeline\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a800583-a6a1-4a32-a54e-ff87b9c84cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_image(dataset,downscale=True,image_size=(299,299)):\n",
    "    \"\"\"Map function to get an array of images from the coco dataset.\"\"\"\n",
    "    img=dataset['image']\n",
    "    \n",
    "    img=tf.image.resize(img,image_size)\n",
    "    if downscale:\n",
    "        img=img/255\n",
    "    return img\n",
    "def get_coco_capts(dataset):\n",
    "    \"\"\"Map function to get an array of captions for every image. \n",
    "    \n",
    "    Coco dataset uses 5 captions per image for most images but some have 6 or 7 \n",
    "    we will crop them to 5 for ease of implementation of batching in the pipeline. \n",
    "    Otherwise tensorflow cannot batch different sizes. \"\"\"\n",
    "    \n",
    "    captions='<start> '+ dataset['captions']['text'][0:5] + ' <end>'\n",
    "    return captions\n",
    "\n",
    "def get_coco_img_paths(dataset):\n",
    "    \"\"\"Map function to get the image paths from original coco dataset.\"\"\"\n",
    "    return dataset['image/filename']\n",
    "def to_dict(image, text):\n",
    "    \"\"\" To be called on dataset.map to get a dictionary\"\"\"\n",
    "    return {'image': image, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88a5c3e2-9a85-4547-ad9b-3e50a7b486ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale=False\n",
    "image_size=(299,299)\n",
    "# Load the dataset and it's info from tfds.\n",
    "data,info = tfds.load(name='coco_captions',with_info=True)\n",
    "\n",
    "# Obtain training images, training captions and training image paths. \n",
    "train_imgs = data['train'].map(lambda x:get_coco_image(x,downscale,image_size), \n",
    "                             num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_capts = data['train'].map(get_coco_capts,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_img_paths=data['train'].map(get_coco_img_paths,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    " # Obtain validation images, captions and image paths. \n",
    "val_imgs = data['val'].map(lambda x:get_coco_image(x,downscale,image_size), \n",
    "                             num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_capts = data['val'].map(get_coco_capts,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_img_paths=data['val'].map(get_coco_img_paths,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Obtain test images, captions and image paths\n",
    "test_imgs = data['test'].map(lambda x:get_coco_image(x,downscale,image_size), \n",
    "                             num_parallel_calls=tf.data.AUTOTUNE) \n",
    "test_capts=data['test'].map(get_coco_capts,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_img_paths=data['test'].map(get_coco_img_paths,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Zip images and captions together \n",
    "train_data_captioning=tf.data.Dataset.zip((train_imgs,train_capts)).\\\n",
    "            map(to_dict,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_data_captioning=tf.data.Dataset.zip((val_imgs,val_capts)).\\\n",
    "            map(to_dict,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_data_captioning=tf.data.Dataset.zip((test_imgs,test_capts)).\\\n",
    "            map(to_dict,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset_dictionary={\n",
    "                      'captioning':{\n",
    "                                    'train': train_data_captioning,\n",
    "                                    'val': val_data_captioning,\n",
    "                                    'test': test_data_captioning,\n",
    "                                    'train_img_paths': train_img_paths,\n",
    "                                    'val_img_paths': val_img_paths,\n",
    "                                    'test_img_paths': test_img_paths,\n",
    "                                    'train_captions': train_capts,\n",
    "                                    'val_captions': val_capts,\n",
    "                                    'test_captions': test_capts\n",
    "                                  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b17664-7349-4cd7-8cd9-78ef43b81330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'<start> A toilet and sink in a tiled bathroom <end>'\n",
      " b'<start> A unisex bathroom decorated with a vintage theme.  <end>'\n",
      " b'<start> A white toilet sitting next to a bidet toilet. <end>'\n",
      " b'<start> A bathroom with a toilet, sink, and other bathroom items in it.  <end>'\n",
      " b'<start> A bathroom with gold circle patterns containing a toilet, sink towel rack and shelving. <end>'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for i in train_capts.take(1):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "myenv",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
