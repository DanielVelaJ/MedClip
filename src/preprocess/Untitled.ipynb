{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48651520-2ff0-4d48-9b4c-249cc7941825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6350140a-ab47-4b5c-ab20-90a90aa3336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_images(path_list):\n",
    "    \"\"\"\n",
    "    Check integrity of images in path.\n",
    "\n",
    "    Args:\n",
    "        path (list of str): list containing the paths to images.\n",
    "\n",
    "    Returns:\n",
    "        bad_images (list of str): list of paths pointing to corrupted images.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ext_list = ['jpg', 'png', 'jpeg', 'gif', 'bmp']\n",
    "    bad_images = []\n",
    "    for f_path in tqdm(path_list):\n",
    "        try:\n",
    "            tip = imghdr.what(f_path)\n",
    "        except:\n",
    "            # print(f_path+' not found')\n",
    "            bad_images.append(f_path)\n",
    "            continue\n",
    "        if ext_list.count(tip) == 0:\n",
    "\n",
    "            bad_images.append(f_path)\n",
    "\n",
    "        if os.path.isfile(f_path):\n",
    "            try:\n",
    "                img = cv2.imread(f_path)\n",
    "                shape = img.shape\n",
    "            except:\n",
    "                # print('file ', f_path, ' is not a valid image file')\n",
    "                bad_images.append(f_path)\n",
    "        else:\n",
    "            pass\n",
    "            # print('could not find file {path}')\n",
    "\n",
    "    return bad_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "104ab64d-97ae-482b-845d-d8b86121be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25805/25805 [03:38<00:00, 118.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing bad images\n",
      "['../data/raw/medpix/Images/258.jpg', '../data/raw/medpix/Images/266.jpg', '../data/raw/medpix/Images/267.jpg', '../data/raw/medpix/Images/268.jpg', '../data/raw/medpix/Images/741.jpg', '../data/raw/medpix/Images/742.jpg', '../data/raw/medpix/Images/903.jpg', '../data/raw/medpix/Images/989.jpg', '../data/raw/medpix/Images/1384.jpg', '../data/raw/medpix/Images/1752.jpg', '../data/raw/medpix/Images/1758.jpg', '../data/raw/medpix/Images/1759.jpg', '../data/raw/medpix/Images/1762.jpg', '../data/raw/medpix/Images/15800.jpg', '../data/raw/medpix/Images/15801.jpg', '../data/raw/medpix/Images/15802.jpg']\n",
      "there are a total of 16 bad images\n"
     ]
    }
   ],
   "source": [
    "# Configure where to save the resulting dataset\n",
    "save_path_dir = '../data/intermediate/'\n",
    "os.makedirs(save_path_dir, exist_ok=True)\n",
    "save_path = save_path_dir+'inter_medpix.csv'\n",
    "\n",
    "# Read the raw df\n",
    "path = '../data/raw/medpix/Dataset_MedPix_V1.xlsx'\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "# Grab useful columns\n",
    "useful_cols=['ID','Plane',\n",
    "             'Core_Modality','Full_Modality',\n",
    "             'Findings','Case_Diagnosis','Location']\n",
    "df=df[useful_cols]\n",
    "\n",
    "# Rename columns\n",
    "renamings={'ID':'Path',\n",
    "           'Full_Modality':'Modality',\n",
    "          'Case_Diagnosis':'Impression',\n",
    "          'Location':'Anatomy'}\n",
    "df.rename(columns=renamings,inplace=True)\n",
    "# Now the dataframe contains columns \n",
    "#['Path', 'Plane', 'Core_Modality', 'Modality', 'Findings', 'Impression',\n",
    "# 'Anatomy']\n",
    "\n",
    "\n",
    "# Drop rows with empty values\n",
    "df = df.dropna() \n",
    "\n",
    "# CLEANING ON THE \"PLANE\" COLUMN-------------------------\n",
    "# Consolidating synonims into single types.\n",
    "df.Plane.replace('Transverse','Axial',inplace=True)\n",
    "df.Plane.replace('Lateral','Sagittal',inplace=True)\n",
    "df.Plane.replace('Frontal','Coronal',inplace=True)\n",
    "\n",
    "# Keep rows that have a plane values with a frequency higher than 100\n",
    "valid_index=df.Plane.value_counts().index[df.Plane.value_counts()>100]\n",
    "df=df.loc[df.Plane.isin(valid_index)]\n",
    "\n",
    "# Drop whenever plane is equal to particular values.\n",
    "df = df.loc[~df.Plane.isin(['NOS - Not specified', #\n",
    "                      'Other View (see caption)'])]\n",
    "\n",
    "# CLEANING ON THE \"CORE_MODALITY\" COLUMN-------------------\n",
    "# Consolidating synonims under the same concept.\n",
    "df.Core_Modality.replace('US-D','US',inplace=True)\n",
    "df.Core_Modality.replace('CTA','AN',inplace=True)\n",
    "df.Core_Modality.replace('MRA','AN',inplace=True)\n",
    "df.Core_Modality.replace('Histology','HE',inplace=True)\n",
    "df.Core_Modality.replace('PET','PET/NM',inplace=True)\n",
    "df.Core_Modality.replace('NM','PET/NM',inplace=True)\n",
    "df.Core_Modality.replace('PET-CT','PET/NM',inplace=True)\n",
    "df.Core_Modality.replace('MRS','MR',inplace=True)\n",
    "\n",
    "# Keep rows that have a Core_Modality values with a frequency higher than 100\n",
    "valid_index=df.Core_Modality.value_counts().index[df.Core_Modality.value_counts()>100]\n",
    "df=df.loc[df.Core_Modality.isin(valid_index)]\n",
    "\n",
    "# Drop whenever plane is equal to particular values.\n",
    "df = df.loc[~df.Core_Modality.isin(['NOS'])]\n",
    "\n",
    "# CLEANING ON \"FINDINGS\" COLUMN------------------------------\n",
    "# Eliminate rows that have a \"findings\" wordcount larger than 100 words. \n",
    "df[\"Number of Words\"] = df[\"Findings\"].apply(lambda n: len(n.split()))\n",
    "df=df.loc[df['Number of Words']<=100]\n",
    "\n",
    "# CLEANING ON THE \"ANATOMY\" COLUMN--------------------------\n",
    "# Consolidation\n",
    "df.Anatomy.replace('Brain and Neuro','Brain',inplace=True)\n",
    "df.Anatomy.replace('Nerve, central','Brain',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('MSK - Musculoskeletal','Musculoskeletal',inplace=True)\n",
    "df.Anatomy.replace('Extremity - Please Select MSK','Musculoskeletal',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('Chest, Pulmonary (ex. Heart)','Pulmonary',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('Breast and Mammography','Breast',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('Abdomen - Generalized','Abdomen',inplace=True)\n",
    "df.Anatomy.replace('Gastrointestinal','Abdomen',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('Head and Neck (ex. orbit)','Head and Neck',inplace=True)\n",
    "df.Anatomy.replace('Eye and Orbit (exclude Ophthalmology)','Head and Neck',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('Vascular','Cardiovascular',inplace=True)\n",
    "df.Anatomy.replace('Cardiovascular (inc. Heart)','Cardiovascular',inplace=True)\n",
    "\n",
    "df.Anatomy.replace('Multisystem','Generalized',inplace=True)\n",
    "\n",
    "# Keep rows that have Anatomy values with a frequency higher than 200\n",
    "valid_index=df.Anatomy.value_counts().index[df.Anatomy.value_counts()>200]\n",
    "df=df.loc[df.Anatomy.isin(valid_index)]\n",
    "\n",
    "# CLEANING THE IMPRESSIONS COLUMN---------------------\n",
    "# Keep impressions with at most 30 words.\n",
    "df[\"Number of Words\"] = df[\"Impression\"].apply(lambda n: len(n.split()))\n",
    "df=df.loc[df['Number of Words']<=30]\n",
    "\n",
    "# Eliminate the Number or words column\n",
    "df.drop(columns='Number of Words').count()\n",
    "\n",
    "# CONVERT THE PATH COLUMN INTO THE COMPLETE PATHS--------------------\n",
    "prefix = '../data/raw/medpix/Images/'\n",
    "suffix = '.jpg'\n",
    "df.Path = prefix+df.Path.astype(str)+suffix\n",
    "\n",
    "# CREATE THE FULL CAPTIONS COLUMN-------------------------------------\n",
    "df['Full_Caption']=df.apply(lambda row: ('<start>'+\n",
    "                                         ' Core Modality:'+ str(row['Core_Modality'])+\n",
    "                                         ' Modality: ' + str(row['Modality'])+\n",
    "                                         ' Plane: ' + str (row['Plane']) +\n",
    "                                         ' Anatomy: ' + str(row['Anatomy'])+\n",
    "                                         ' Findings: '+ str(row['Findings'])+\n",
    "                                         ' Impression: '+ str(row['Impression'])+' <end>') ,axis=1)\n",
    "\n",
    "# CHECK THAT WE ARE ABLE TO OPEN IMAGES POINTED BY THE PATH COLUMN------------------\n",
    "bad_images=check_images(df.Path.to_list())\n",
    "print('listing bad images')\n",
    "print(bad_images)\n",
    "print(f'there are a total of {len(bad_images)} bad images')\n",
    "df=df.loc[~df.Path.isin(bad_images)] # eliminate rows with bad images from dataframe\n",
    "df.to_csv(save_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "myenv",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
