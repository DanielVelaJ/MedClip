# -*- coding: utf-8 -*-
"""
Created on Tue Jun 28 14:03:11 2022

@author: Daniel Vela Jarquin

This module contains the functions to generate pipelines ready for modeling.
it requires the following to work:
    1. The raw folder has the unzipped datasets. This is done by the script
        download_data
    2. The prepared datasets are ready. These are generated by the script
        prepare_data

"""
# TODO:
#     Finish the make_split function
#     Implement splits in make_pipeline function
    
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

def make_split(df_size,dataset, shuffle=True, seed=1):
    """
    Make train, 
    """

def make_pipeline(inter_dataset_path,
                  image_size=(299, 299),downscale=True):
    """
    Generates model_ready training data.

    Takes a prepared (generated by functions in the prepare.py module) dataset
    and builds a dictionary with keys, CLIP, labeling, captioning. The values
    are the datasets in tensorflow format, necessary to run each of the
    pretraining tasks.




    Args:
        inter_dataset_path (str): The path to an intermediate dataset
        image_shape(touple): Size used to reshape images.

    Returns:
        datasets_dict(dictionary): A dictionary containing tensorflow datasets.
        The key CLIP contains a tf dataset consisting of elements:
            ({image,text},)
        The key captioning contains a tf dataset consisting of elements:
            (image,caption)
        They key labeling contains a tf dataset consisting of elements:
            (image,labels)

     """
    print('making input pipeline')
    df = pd.read_csv(inter_dataset_path)

    def decode_and_resize(img_path,scale):
        """Recieves an image path, decodes and rescales it. """
        img = tf.io.read_file(img_path)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, image_size)
        img = tf.image.convert_image_dtype(img, tf.float32)
        img = img*scale
        return img

    def to_dict(image, text):
        """ To be called on dataset.map to get a dictionary"""
        return {'image': image, 'text': text}

    # Make a dataset of images
    image_paths = df['Path']
    # Set whether to leave pixel values between 0-255 or from 0-1
    if downscale==True:
        scale = float(1/255)
    else:
        scale = 1
    tf.data.Dataset.from_tensor_slices(image_paths).map(lambda x: decode_and_resize(x,scale),
                                                                         num_parallel_calls=tf.data.AUTOTUNE)
    # Make a dataset with captions
    if ('Findings' in df.columns):
        # If captions are available
        captions = df['Findings'].astype(str)
        captions_dataset = tf.data.Dataset.from_tensor_slices(captions)
    else:
        # If captions are not available
        print("Prepared dataframe doestn't contain key: Findings")
        captions_dataset = None

    # Make a dataset of labels
    label_cols = [col for col in df.columns if 'label' in col]
    if (len(label_cols) != 0):
        # If there are labels in the dataset
        labels_dataset = tf.data.Dataset.from_tensor_slices(df[label_cols])
    else:
        labels_dataset = None

    # Generate the proper structure for each pretraining task
    # Proper structure for CLIP
    if (captions_dataset is not None):
        # If there are captions available
        clip_dataset = tf.data.Dataset.zip((images_dataset, captions_dataset))
        clip_dataset = clip_dataset.map(to_dict)
        clip_dataset = clip_dataset.zip((clip_dataset,))
    else:
        clip_dataset = None

    # Proper structure for labeling
    if (labels_dataset is not None):
        # If there are labels available
        labeling_dataset = tf.data.Dataset.zip(
            (images_dataset, labels_dataset))
    else:
        labeling_dataset = None

    # Proper structure for Captioning
    if (captions_dataset is not None):
        # If there are captions available
        captioning_dataset = tf.data.Dataset.zip(
            (images_dataset, captions_dataset))
    else:
        captioning_dataset = None

        
        
    # Make splits from built datasets
    dataset_size = len(df)
    if (captions_dataset is not None):
        captions_dataset_train=captions_dataset.take(int(0.7*dataset_size))
        captions_dataset_val=captions_dataset.skip(int(0.7*dataset_size)).take(0.15*dataset_size)
        
    
    
    return {'CLIP': clip_dataset,
            'captioning': captioning_dataset,
            'labeling': labeling_dataset}
