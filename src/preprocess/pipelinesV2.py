# -*- coding: utf-8 -*-
"""
Created on Tue Jun 28 14:03:11 2022

@author: Daniel Vela Jarquin

This module contains the functions to generate pipelines ready for modeling.
it requires the following to work:
    1. The raw folder has the unzipped datasets. This is done by the script
        download_data.py
    2. The prepared datasets are ready. These are generated by the script
        prepare_data.py

"""
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import numpy as np

def decode_and_resize(img_path,image_size):
    """Recieves an image path, decodes and rescales it. """
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, image_size)
    img = tf.image.convert_image_dtype(img, tf.float32)
    return img

def to_dict(image, text):
    """ To be called on dataset.map to get a dictionary"""
    return {'image': image, 'text': text}

def build_tf_dataset(df_subset,image_size):
    """Build captioning tensorflow dataset from dataframe
    
    The captioning tensorflow dataset returns dictionaries
    containing "images" and "text" keys.
    
    Args: 
        df_subset(pandas df): The dataframe from which to extract
            imagepaths and captions. It must contain the columns
            "Full_Caption" and "Path"
    Returns
        captioning_tf_dataset(tf dataset): Captioning tensorflow dataset. 
    """
    # Build images tf dataset
    images_tf_dataset = tf.data.Dataset.from_tensor_slices(df_subset['Path'])\
                            .map(lambda x: decode_and_resize(x,image_size),num_parallel_calls=tf.data.AUTOTUNE)

    # Build captions tf_dataset
    captions_tf_dataset = tf.data.Dataset\
                              .from_tensor_slices(df_subset['Full_Caption'].astype(str))

    # Zip df_subset images and captions datasets into a captioning dataset. 
    captioning_tf_dataset = tf.data.Dataset.zip(
                                                    (images_tf_dataset, 
                                                     captions_tf_dataset.map(lambda x:tf.expand_dims(x,axis=0)))
                                                  ).map(to_dict)
    return captioning_tf_dataset

def build_pipeline(inter_dataset_path='../data/intermediate/inter_medpix.csv',
                   image_size=(299, 299),
                   train_val_test_split=[0.70,0.15,0.15]):
    """ Build a captioining pipeline.
    
    Args:
        inter_dataset_path(str):path to the intermediate dataset. 
        image_size(tuple): size for image resizing. 
        train_val_test_split: the fraction of data to use for training
            validation and test sets.

    """
    df = pd.read_csv(inter_dataset_path) # Read csv
    df = df.sample(frac=1,random_state=1).reset_index() # shuffle df

    # Split into train validation and test splits------------------------------
    n=len(df) # Number of samples in the df
    train_n = int(train_val_test_split[0]*n) # Number of samples in training set
    val_n = int(train_val_test_split[1]*n)   # Number of samples in val set
    test_n = n-train_n-val_n                 # Number of samples in test set

    # Make sure there are no duplicates in the validation and test sets.
    # unless we are using the chexpert dataset which has many duplicates. 

    # If we are not using the chexpert dataset
    if 'chexpert' not in inter_dataset_path:

        uniques=df[~df.duplicated('Caption',keep=False)]   # unique values
        duplicated=df[df.duplicated('Caption',keep=False)] # duplicated values
        # Output for debugging duplicated captions
        # uniques.to_csv('uniques.csv')
        # duplicated.to_csv('duplicated.csv')

        # If there are enough unique values to fill the validation and test sets:
        if len(uniques)>=(val_n+test_n):
            val_df=uniques[0:val_n]                  # Populate validation df
            test_df=uniques[val_n:val_n+test_n]      # Populate test df
            remaining_uniques=uniques[val_n+test_n:] # Unique samples not used so far
            # Concatenate remaining uniques with duplicates
            train_df=pd.concat([remaining_uniques,duplicated],axis=0)  
            train_df=train_df.sample(frac=1,random_state=1)\
                           .reset_index(drop=True)   # Shuffle train_df again. 
        else:
        # If there are not enough samples print error message
            print('not enough unique values to generate these many validation and test samples')
            exit()
    else:
    # If using the chexpert dataset populate despite duplicates 
        train_df=df[0:train_n]
        val_df=df[train_n:train_n+val_n]
        test_df=df[train_n+val_n:train_n+val_n+test_n]

    # Generate tensorflow datasets for training, validation and test sets.-------------
    train_captioning_tf_dataset = build_tf_dataset(train_df,image_size) # Generate training tensorflow dataset from training df
    val_captioning_tf_dataset = build_tf_dataset(val_df,image_size)     # Generate validation tensorflow dataset from validation df
    test_captioning_tf_dataset = build_tf_dataset(test_df,image_size)   # Generate test tensorflow dataset from test df

    pipeline = {'captioning':{
                                'train':train_captioning_tf_dataset,
                                'val':val_captioning_tf_dataset,
                                'test':test_captioning_tf_dataset,
                                'train_captions':train_df['Full_Caption'].astype(str).to_list()
                             }
                } 
    return pipeline
    
def build_coco_pipeline(downscale=False,image_size=(299,299),tokenizer=None):
    """
    Generates model_ready training data from the coco dataset.

    Takes the tensorflow datasets instance of the "coco captions" dataset
    and builds a dictionary with keys, 'CLIP' and 'captioning'. The values
    are the datasets (in tensorflow format), necessary to run each of the
    pretraining tasks.




    Args:
        downscale(bool,optional): Whether to divide image values by 255. 
            Defaults to True.
        image_size(touple,optional): Size used to reshape images. Defaults to 
            (299,299)
    

    Returns:
        datasets_dict(dictionary): A dictionary containing tensorflow datasets.
            The key CLIP contains a tf dataset consisting of elements:
                ({image,text},)
            The key captioning contains a tf dataset consisting of elements:
                (image,caption)
            
            There is also a dataset which includes the image paths for each 
            train, val and test split. 
            

     """
    print('Loading coco...')
    print('remember coco dataset has 5 captions per image so',
          ' the value for the "text" key will be a (5,) string array.')
    
    # Load the dataset and it's info from tfds.
    data,info = tfds.load(name='coco_captions',with_info=True)
    
    # Obtain training images, training captions and training image paths. 
    train_imgs = data['train'].map(lambda x:get_coco_image(x,downscale,image_size), 
                                 num_parallel_calls=tf.data.AUTOTUNE)
    train_capts = data['train'].map(get_coco_capts,num_parallel_calls=tf.data.AUTOTUNE)
    train_img_paths=data['train'].map(get_coco_img_paths,num_parallel_calls=tf.data.AUTOTUNE)

     # Obtain validation images, captions and image paths. 
    val_imgs = data['val'].map(lambda x:get_coco_image(x,downscale,image_size), 
                                 num_parallel_calls=tf.data.AUTOTUNE)
    val_capts = data['val'].map(get_coco_capts,num_parallel_calls=tf.data.AUTOTUNE)
    val_img_paths=data['val'].map(get_coco_img_paths,num_parallel_calls=tf.data.AUTOTUNE)

    # Obtain test images, captions and image paths
    test_imgs = data['test'].map(lambda x:get_coco_image(x,downscale,image_size), 
                                 num_parallel_calls=tf.data.AUTOTUNE) 
    test_capts=data['test'].map(get_coco_capts,num_parallel_calls=tf.data.AUTOTUNE)
    test_img_paths=data['test'].map(get_coco_img_paths,num_parallel_calls=tf.data.AUTOTUNE)

    # Zip images and captions together 
    train_data_captioning=tf.data.Dataset.zip((train_imgs,train_capts)).\
                map(to_dict,num_parallel_calls=tf.data.AUTOTUNE)
    val_data_captioning=tf.data.Dataset.zip((val_imgs,val_capts)).\
                map(to_dict,num_parallel_calls=tf.data.AUTOTUNE)
    test_data_captioning=tf.data.Dataset.zip((test_imgs,test_capts)).\
                map(to_dict,num_parallel_calls=tf.data.AUTOTUNE)
    



    dataset_dictionary={
                          'captioning':{
                                        'train': train_data_captioning,
                                        'val': val_data_captioning,
                                        'test': test_data_captioning,
                                        'train_img_paths': train_img_paths,
                                        'val_img_paths': val_img_paths,
                                        'test_img_paths': test_img_paths,
                                        'train_captions': train_capts,
                                        'val_captions': val_capts,
                                        'test_captions': test_capts
                                      }
                         }
    return dataset_dictionary

def get_coco_image(dataset,downscale=True,image_size=(299,299)):
    """Map function to get an array of images from the coco dataset."""
    img=dataset['image']
    
    img=tf.image.resize(img,image_size)
    if downscale:
        img=img/255
    return img
def get_coco_capts(dataset):
    """Map function to get an array of captions for every image. 
    
    Coco dataset uses 5 captions per image for most images but some have 6 or 7 
    we will crop them to 5 for ease of implementation of batching in the pipeline. 
    Otherwise tensorflow cannot batch different sizes. """
    
    # Add start and end tokens
    captions='<start> '+ dataset['captions']['text'][0:5] + ' <end>'
    return captions

def get_coco_img_paths(dataset):
    """Map function to get the image paths from original coco dataset."""
    return dataset['image/filename']

def tokenize(ds_row,tokenizer):
    tokens=tokenizer.tokenize(ds_row['text'])
    return {'image':ds_row['image'],
           'text':ds_row['text'],
           'tokens':tokens}