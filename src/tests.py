# =============================================================================
# # -*- coding: utf-8 -*-
# """
# Created on Mon Jun 27 19:09:15 2022
#
# @author: danic
# """
#
#
# """
# Generates model_ready training data.
#
# This function takes a inter_dataset (generated by the other 'prepare'
# functions in this module) or the path to the inter_dataset and then
# generates a tensorflow datasets object consisting of a dictionary with
# the following keys:
#
#
#     image: Image of the shape spacified by image_shape
#
#     labels: The labels that correspond to the image (one_hot vector)
#
#     caption: Capion corresponding to the image
#
#
# Args:
#     inter_dataset_path (str): The path to an intermediate dataset
#     image_shape(touple): Size used to reshape images.
#
# Returns:
#     dataset(tensorflow dataset): a tensorflow dataset object.
#     info: a dictionary containing info about the dataset
#
#  """
#
#
# # Parameters
# import pandas as pd
# import tensorflow as tf
# import matplotlib.pyplot as plt
# inter_dataset_path = 'C:/Users/danic/MedClip/data/intermediate/inter_chexpert.csv'
# IMAGE_SIZE = (299, 299)
#
#
# df = pd.read_csv(inter_dataset_path)
#
#
# def decode_and_resize(img_path):
#     img = tf.io.read_file(img_path)
#     img = tf.image.decode_jpeg(img, channels=3)
#     img = tf.image.resize(img, IMAGE_SIZE)
#     img = tf.image.convert_image_dtype(img, tf.float32)
#     img = img/255
#     return img
#
#
# def to_dict(image, text):
#     return {'image': image, 'text': text}
#
#
# # Make a dataset of images
# image_paths = df['Path']
# images_dataset = tf.data.Dataset.from_tensor_slices(image_paths).map(decode_and_resize,
#                                                                      num_parallel_calls=tf.data.AUTOTUNE)
#
# # Make a dataset with captions
# captions = df['Findings']
# captions_dataset = tf.data.Dataset.from_tensor_slices(captions)
#
#
# # Make a dataset of labels
# label_cols = [col for col in df.columns if 'label' in col]
# labels_dataset = tf.data.Dataset.from_tensor_slices(df[label_cols])
#
# # Generate the proper structure for each pretraining task
#
# # Proper structure for CLIP
# clip_dataset = tf.data.Dataset.zip((images_dataset, captions_dataset))
# clip_dataset = clip_dataset.map(to_dict)
# clip_dataset = clip_dataset.zip((clip_dataset, labels_dataset))
#
#
# # Proper structure for labeling
# clip_dataset = tf.data.Dataset.zip((images_dataset, labels_dataset))
#
# # Proper structure for Captioning
# capt_Dataset = tf.data.Dataset.zip((images_dataset, captions_dataset))
#
# =============================================================================
# from preprocess import pipelines
# medpix_path = 'C:/Users/danic/MedClip/data/intermediate/inter_medpix.csv'
# medpix_dict = pipelines.make_pipeline(medpix_path)

# chexpert_path = 'C:/Users/danic/MedClip/data/intermediate/inter_chexpert.csv'
# chexpert_dict = pipelines.make_pipeline(chexpert_path)


# TESTS FOR LABELING PRETRAINING
from pretraining  import pretrain
from preprocess import pipelines
data_pipeline=pipelines.make_pipeline('../data/intermediate/inter_chexpert.csv')
pretrain.labeling(data_pipeline,100,debug=True)



# TEST FOR .SH within python
# import subprocess
# print("start")
# subprocess.call("./script.sh",shell=True)
# print("end")